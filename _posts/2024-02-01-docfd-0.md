---
layout: post
title:  "Docfd: Making a Pretty Good TUI Multiline Fuzzy Document Finder (WIP)"
date:   2024-02-01 16:27:03 +1100
categories: docfd foss
---

# Introduction

Docfd is a TUI multiline fuzzy document finder with text editor and PDF viewer integration,
allowing you to jump directly to a search result with a single key press.

![](demo-vhs-gifs/repo.gif)

We can roughly think of it as an interactive grep for both text files and PDFs,
but word/token based instead of regex and line based.

Docfd was initially made out of frustration of not having a simple way
to navigate a collection of text files.
fzf and grep (including ripgrep, ugrep, etc)
suffice up to an extent, but they are really designed for line
based input, not paragraph based input.
And, well, we don't typically write everything in a single line,
and we can't predict how a phrase might be split across lines, or even if
it's split across consecutive lines with no gaps inbetween.

The word/token based approach
makes the problem of searching across multiple lines much easier,
as there is no meaningful difference between searching
in the same line versus multiple lines for a list of words/tokens.

The simplification obviously comes at the cost of expressiveness,
but for me I've concluded that I do very basic search most of
the time and don't end up using
regex at all.

As for searching through PDFs,
it came later after a user's suggestion.
I was initially hesitant in adding PDF support
as I predicted it to be a lot of work,
but ultimatley convinced myself it was worth the effort.
And it was! Both a lot of work and worth the effort.

# Background

## Fuzzy finding

Fuzzy finding is just a colloquial term
for approximate string matching,
where there's a notion of "closeness"
between strings defined as edit distance.
It is of interest to us
because it presents
a low overhead way for user to specify
search query that is tolerant
to minor differences in spelling and typos.

Roughly speaking, edit distance is measure of how many
character based operations are required
to go from one string to the other. For instance,
the following pair of words are one distance away
from each other:

- `cat` and `coat` - we can insert `o` into `cat` to get `coat`
- `coat` and `coal` - we can replace `t` with `l` in `coat` to get `coal`

The exact operations
and definitions will depend on the algorithm discussed,
but it suffices here to just be aware that we have some
concrete means
of measuring the proximity of words.
This is because we are mostly interested in using the matching
algorithms rather than investigating and
comparing characteristics such as performance.

Fuzzy finders then just refer to tools that
make use of this as part of their core functionality.

## Regex, multiline and fuzziness

Regex, or regular expression, is a common way to specify
patterns for matching strings. We see if very often for
matching file names, matching content, and so on.
For our purpose of explanation,
the wildcard syntax `.*` (depending
on the tool, it may also be written as just `*`)
suffices.

Suppose we want to look for a phrase that has
the words `evaluate` and `expression`,
we can try using the pattern `evaluate.*expression` to
denote this query.
But this runs into the problem of requiring both
words to appear in the same line.
We can see this problem with following line outputting nothing

```
$ echo -e "evaluate\nexpression" | grep "evaluate.*expression"
```

We can work around this by, for instance, using ripgrep
with multiline flag, or using ugrep that accepts multiline out of the box

```
$ echo -e "evaluate\nexpression" | ugrep "evaluate.*\n.*expression"
```

However, this requires stating up front a bound of how many lines inbetween
the two words we expect.
For the usual markdown or text files, we can argue expecting just one newline
character inbetween is sufficient.
But this does not leave a lot of flexibility for accommodating blank lines
or differently structured text, e.g. extracted text from PDFs.

The alternative of simply allowing `.*` to gloss
over newline characters is not desirable either -
we may end up needing to keep thousands or million
lines in memory before we find a match.

We can argue that we just insert a bound of, say, 5 lines
between each word and call it a day,
i.e. `evaluate.*\n.*\n.*\n.*\n.*\n.*expression`,
but this
does not account for pathological cases where we might have
very long lines, and search results with 1000 words
between "evaluate" and "expression" is not very useful.

On a somewhat similar note, fuzziness does not
translate directly into regex.
We can obviously programatically generate the appropriate
regex accounting for edit distance,
but then we are really just using the wrong tool
if it requires this many roundabout intermediate
layers.

# Core components

## Phrase search

## Search result scoring

# TUI layer

# Integration with external tools

## Text editor integration

## PDF viewer integration

# Final thoughts
